## Evaluation Plan

The evaluation aims to demonstrate the practical effectiveness, overhead, coverage, and anomaly-detection capabilities of the **AgentSight** observability system in realistic production-like scenarios.

### Evaluation Goals

1. **Coverage & Completeness:**
   Does AgentSight comprehensively capture agent interactions, subprocesses, and semantic behaviors in production-like conditions?

2. **Performance Overhead:**
   How much runtime overhead (CPU, latency) does AgentSight introduce into production workloads?

3. **Detection Effectiveness:**
   How effectively can AgentSight detect anomalies such as prompt injection, unauthorized subprocess execution, reasoning loops, or semantic drift?

4. **Robustness & Stability:**
   Can AgentSight maintain stable observation over prolonged operational periods without significant data loss or disruption?

---

## Practical Research Questions & Methodology

### Q1: Coverage & Completeness

* **RQ 1.1:** Can AgentSight accurately trace cross-process agent interactions and tool executions without explicit application instrumentation?
* **RQ 1.2:** Does AgentSight reliably decrypt TLS payloads and capture semantic information in realistic ML-agent communication patterns?

**Methodology:**

* Deploy multiple representative agent workflows (LangChain, AutoGen, Claude-Code, Gemini-CLI) involving subprocess invocations, external tool executions (e.g., shell scripts, curl), and TLS communication.
* Measure and report the proportion of correctly captured events against known injected behaviors.

---

### Q2: Performance Overhead

* **RQ 2.1:** What is the CPU overhead introduced by kernel-level tracing of TLS buffers, syscalls, and subprocesses?
* **RQ 2.2:** How does AgentSight affect latency and throughput of AI-agent-driven workloads under realistic conditions?

**Methodology:**

* Deploy workloads on representative infrastructure (e.g., AWS EC2 instances or containerized Kubernetes clusters).
* Measure baseline performance without tracing; repeat measurements with AgentSight enabled.
* Collect data on CPU utilization, latency, and throughput changes.

---

### Q3: Detection Effectiveness

* **RQ 3.1:** Can AgentSight identify subtle anomalies such as semantic drift, reasoning loops, unauthorized subprocess execution, and prompt injections?
* **RQ 3.2:** How accurate and actionable are the alerts and anomaly signals generated by AgentSight?

**Methodology:**

* Introduce known anomalies deliberately into agent workloads:

  * Malicious or unintended prompt injections
  * Unauthorized curl or subprocess calls
  * Intentional infinite reasoning loops
  * Sudden persona shifts or semantic contradictions
* Evaluate detection rate, false-positive/negative rates, and time-to-detection (TTD).

---

### Q4: Robustness & Stability

* **RQ 4.1:** Does AgentSight operate consistently without disruption or significant degradation over extended monitoring periods (24â€“72 hours)?
* **RQ 4.2:** How stable is the eBPF instrumentation against kernel version changes or diverse Linux distributions?

**Methodology:**

* Perform prolonged monitoring runs (24â€“72 hours) under realistic workload conditions.
* Record trace data loss rates, memory consumption, and operational interruptions.
* Test across multiple Linux kernel versions and distributions.

---

## Evaluation Metrics & Summary Table

| Dimension         | Metrics                                                                                     |
| ----------------- | ------------------------------------------------------------------------------------------- |
| **Coverage**      | Event capture rate (%), false negative rate (%)                                             |
| **Performance**   | CPU overhead (%), latency overhead (ms, % increase), throughput degradation (%)             |
| **Effectiveness** | Anomaly detection accuracy (%), false positives (%), false negatives (%), time-to-detection |
| **Robustness**    | Data loss rate (%), uptime (%), memory footprint (MB)                                       |

---

## Experimental Scenarios and Workloads

To ensure practical relevance, select scenarios involving:

* **Software engineering agents:**

  * Claude-Code, AutoGen, Gemini-CLI performing code generation tasks, auto-debugging, and deployment scripts.

* **Data-analysis pipelines:**

  * LangChain-based retrieval-augmented generation (RAG) agents interacting with vector databases, external APIs, and local subprocesses.

* **Security-focused scenarios:**

  * Agents deliberately subjected to prompt-injection attacks, subprocess-based escalation attempts (writing and executing malicious bash scripts), or unauthorized network activity.

---

## Concrete Evaluation Steps (Actionable)

**Step 1: Baseline Measurement**

* Run workflows without AgentSight (establish baseline metrics).

**Step 2: AgentSight Deployment**

* Deploy AgentSight alongside the same workflows, measure coverage, completeness, and performance overhead.

**Step 3: Anomaly Injection**

* Introduce known anomalies (prompt injection, subprocess misuse, reasoning loops).
* Measure detection effectiveness and time-to-detection.

**Step 4: Robustness and Stability Test**

* Continuous monitoring for extended periods; collect stability and reliability metrics.

**Step 5: Multi-environment Tests**

* Repeat tests across multiple kernel versions and Linux distributions to test robustness and generalizability.

---

## Expected Outcomes and Impact

* **Comprehensive validation** of AgentSightâ€™s ability to capture ML-driven system-level semantics, enhancing traditional AIOps.
* **Quantified proof** of low-overhead and reliable operation, demonstrating readiness for production use.
* Identification of strengths and limitations clearly documented, guiding future research and development directions.

This evaluation plan and these research questions provide a clear, actionable path for demonstrating AgentSightâ€™s practical utility, robustness, and contribution to ML-powered infrastructure observability within real-world production environments, especially within coding-heavy contexts.

If you're short on time (especially for a workshop paper), the single most important evaluation to prioritize is:

**"Coverage & Detection Effectiveness of Critical AI-Agent Anomalies"**

---

### ðŸ”‘ Why this experiment matters most:

Your paperâ€™s core selling point is highlighting a unique gap in current observability solutions (inability to observe and detect subtle, malicious, or semantically problematic agent behaviors). The value of **AgentSight** lies precisely in capturing what traditional instrumentation missesâ€”prompt injection, malicious subprocess executions, reasoning loops, and semantic drift.

Demonstrating clearly that your approach can **detect anomalies that existing methods cannot** provides the strongest immediate value and generates significant interest at workshops like PACMI.

---

### âœ… Minimum necessary evaluation plan:

If you can run only one focused experiment, do the following simplified scenario:

#### **Step-by-step simplified experiment:**

1. **Set Up Baseline (minimal):**

   * Run a representative AI-agent workload (e.g., Claude-Code, AutoGen, or Gemini-CLI performing software engineering tasks).
   * Confirm no alerts or issues in normal conditions.

2. **Inject Specific, High-Impact Anomalies:**

   * **Prompt Injection**: Agent is prompted to secretly execute a subprocess (e.g., `curl` a remote URL).
   * **Unauthorized Tool Execution**: Agent writes a potentially harmful bash script (`malicious.sh`) and executes it.
   * **Reasoning Loop**: Agent instructed into an infinite reasoning loop scenario.

3. **Detection Capability:**

   * Show AgentSight clearly capturing:

     * TLS payload with injected malicious prompt.
     * Execution of unauthorized subprocess (`curl` or `bash script execution`).
     * Repeated reasoning events indicating loops or drift.

4. **Report Metrics Clearly:**

   * Whether AgentSight detected each injected anomaly (Yes/No).
   * Time-to-detection (seconds) after anomaly occurs.
   * Clearly state that traditional methods (instrumentation-based) would miss these scenarios.

---

### ðŸ“Œ What specifically to include in results (minimum required):

| Injected Anomaly           | Detected by AgentSight? | Time-to-Detection (TTD) |
| -------------------------- | ----------------------- | ----------------------- |
| Prompt Injection           | âœ… Yes                   | \~2 seconds             |
| Unauthorized Exec (`curl`) | âœ… Yes                   | <1 second               |
| Reasoning Loop             | âœ… Yes                   | \~5 seconds             |

**Note**: Precise numbers above are illustrativeâ€”actual measurements required.

---

### ðŸš© Why skip other experiments initially?

* **Performance overhead (CPU/memory)** is less critical for early-stage workshops where you primarily showcase conceptual correctness.
* **Robustness & stability** evaluations (long runs, multi-kernel compatibility) are important, but less urgent for a short paper/demo.
* Comprehensive metrics like throughput or latency overhead can be briefly acknowledged as future work.

---

### ðŸŽ¯ Bottom line recommendation (if very limited):

**Prioritize demonstrating the novelty and effectiveness of AgentSightâ€™s core anomaly detection capabilities.**

This evaluation directly supports your key claims, highlights your novel contribution clearly, and generates maximum interest at an early workshop stage, laying a solid foundation for more detailed future studies.
