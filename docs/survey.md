# Survey of Observability, Tracing, and Debugging in LLMs and Agentic Systems

## Abstract

Large Language Models (LLMs) and LLM-based agent systems pose new challenges for observability, debugging, tracing, and profiling. These AI systems often operate as “black boxes,” making it difficult to understand their internal reasoning or pinpoint the causes of errors. This survey reviews **over fifty recent works (2022–2025)** that explore techniques to monitor and debug LLM-centric systems. We survey both system-level perspectives – such as instrumentation frameworks, telemetry architectures, runtime introspection APIs, and performance profiling tools – and AI-level perspectives – such as metrics for LLM output quality, agent transparency mechanisms, error attribution methods, and taxonomies of agent reasoning steps. We analyze emerging tools and frameworks for prompt tracing, agent logging, and multi-agent observability, drawing parallels to classical software engineering practices: e.g. distributed tracing (OpenTelemetry), OS-level instrumentation (eBPF), assertion-based debugging, deterministic replay, chaos engineering, and sampling-based profiling. From this literature, we distill key challenges in making LLM and multi-agent systems more **observable** and **debuggable** – including the opacity of model “mental state,” the nondeterminism of generation, and the lack of standardized telemetry for AI behaviors. We highlight open research directions such as *cognitive tracing* of LLM reasoning, robust error diagnosis across agents, and unified observability standards for AI. By bridging concepts from traditional software observability with the unique needs of AI systems, the survey outlines a roadmap toward safer, more transparent, and more reliable LLM-based applications.

## Introduction

Large language models have rapidly become core components in software applications, from conversational assistants to autonomous agents. However, **observability** – the ability to monitor and understand system internals – has not kept pace with the complexity of LLM-driven systems. Conventional software observability relies on logs, metrics, and distributed traces[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Observability%20techniques%20applied%20to%20traditional,Such%20methods%20are%20effective), but LLM-based systems present novel challenges. These models carry out complex reasoning in latent high-dimensional spaces, making their decision processes largely invisible to developers. When an LLM-powered agent produces a faulty output or exhibits unexpected behavior, developers often lack insight into *why* it happened or *how* to fix it. As Hassan *et al.* (2024) put it, foundation-model software (or “FMware”) introduces fundamentally new engineering challenges, **observability** being a key one[aiwarebootcamp.ioaiwarebootcamp.io](https://www.aiwarebootcamp.io/slides/2024_aiwarebootcamp_hassan_rethinking_software_and_se_in_the_fm_era.pdf#:~:text=C7%3A%20Operation%20%26%20semantic%20observability,C5%3A%20Compliance%20%26%20regulations%20Conclusion). In multi-agent LLM systems – where multiple LLM instances or tools interact – the complexity grows further, and “without good observability, you’re essentially flying blind”[medium.com](https://medium.com/@kpetropavlov/observability-in-multi-agent-llm-systems-telemetry-strategies-for-clarity-and-reliability-fafe9ca3780c#:~:text=Introduction) in trying to debug such a system.

Recent years have seen a surge of interest in techniques for making LLMs and AI agents more transparent and debuggable. Researchers from both academia and industry are contributing tools to log **prompt traces**, record intermediate reasoning, measure model quality, and monitor performance in real time. For example, new **LLM observability platforms** (LangSmith, Langfuse, Arize Phoenix, etc.) integrate with application frameworks to trace each step of an LLM application – capturing inputs, outputs, latencies, and errors[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=1,tools%20support%20root%20cause%20analysis)[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=,OpenAI%2C%20Anthropic). At the same time, the AI research community is developing methods to *probe the internal states* of models (e.g. activations, attention patterns) to understand their “thought process”[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=it%20only%20answers%20questions%20when,as%20models%20grow%20more%20sophisticated). There is also emerging work on *debugging by prompting*: using the LLM itself to explain or refine its outputs when errors are detected[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=tests%20and%20program%20verifiers%20into,a%20novel%20debugging%20framework).

This survey provides a comprehensive review of literature from 2022–2025 on **observability, debugging, tracing, and profiling** for LLMs and LLM-based agents. We cover two intertwined perspectives:

- **System-Level Observability:** Techniques to instrument and monitor the *computational systems* housing LLMs. This includes telemetry frameworks (e.g. using **OpenTelemetry** to collect traces[last9.io](https://last9.io/blog/langchain-observability/#:~:text=To%20make%20chain%20executions%20observable%2C,errors%20tied%20to%20each%20request)), runtime introspection hooks (akin to using **eBPF** in operating systems for fine-grained monitoring), performance profilers for model execution[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion), and data schemas for logging complex AI workflows. We examine how traditional observability tools are being adapted (or need adaptation) for the unique workloads of large models.
- **AI-Level Observability:** Techniques to expose and analyze the *behavior and reasoning* of the models and agents themselves. This covers defining new metrics for LLM output quality (truthfulness, coherence, bias, etc.), methods for making an agent’s decision process transparent (e.g. chain-of-thought tracing[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,its%20fake%20reasoning%2C%20providing%20a), cognitive frameworks like “Watson” that reconstruct implicit reasoning[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive)), approaches for attributing errors or outputs back to root causes (e.g. identifying which prompt or knowledge source led to a mistake), and taxonomies to categorize agent operations (for instance, classifying spans of an agent’s execution into types like prompting, tool use, waiting, etc.).

By linking these perspectives, we highlight how improving **observability** of LLM systems often requires advances on both fronts. For example, logging an agent’s chain-of-thought (AI-level) may rely on a robust instrumentation of the LLM execution (system-level) to capture those thoughts without disrupting the system. We draw parallels to analogous problems in classic software and distributed systems. Observing an LLM agent’s reasoning can be compared to *distributed tracing* in microservices: each “span” in a trace could correspond to a step in the agent’s plan. Tools like OpenTelemetry have been proposed to trace LLM applications similarly to microservice requests[last9.io](https://last9.io/blog/langchain-observability/#:~:text=Add%20Tracing%20to%20FastAPI%20Endpoints). Likewise, just as **profilers** sample a program’s execution to find performance hotspots, researchers are beginning to profile large models to find efficiency bottlenecks[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion) or even “which neurons or heads” are most used.

The remainder of this paper is organized as follows. **Section 2 (Related Work)** situates our survey in the context of prior surveys and frameworks, clarifying how our focus on LLM observability differs from general MLOps or interpretability surveys. **Section 3 (Technical Dimensions)** delves into specific techniques and tools, first from a system architecture viewpoint (3.1) and then from an AI behavior viewpoint (3.2). We review concrete examples such as tracing libraries, logging schemas, debugging approaches (like inserting assertions or performing *deterministic replays* of LLM decisions), and cross-agent observability in multi-LLM ecosystems. **Section 4 (Challenges)** synthesizes common obstacles noted across the literature – for example, the difficulty of capturing *implicit* reasoning that an LLM doesn’t externalize, the tension between observability and privacy[ijlrp.com](https://www.ijlrp.com/papers/2024/12/1319.pdf#:~:text=,which%20can%20raise%20significant), and the evaluation of non-deterministic AI outputs. **Section 5 (Future Directions)** identifies promising research avenues, from integrating *interpretability methods* into real-time monitoring[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=it%20only%20answers%20questions%20when,as%20models%20grow%20more%20sophisticated) to developing standardized observability benchmarks for AI. We conclude with a call for a new interdisciplinary approach – what some have started calling *“Agentic Observability”*, i.e. adapting software observability to agentic AI systems – to build **trustworthy, transparent, and reliable LLM-based software**.

## Related Work

**Observability and Debugging in Traditional Software:** Observability is a well-established concept in software engineering and systems design. It refers to the ability to infer the internal state of a system from its external outputs. In practice, this is achieved via logging, metrics, and traces in production systems[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Observability%20techniques%20applied%20to%20traditional,Such%20methods%20are%20effective). Tools like **Dapper** and **OpenTelemetry** pioneered *distributed tracing* for microservices, where a unique trace ID links events across services[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Observability%20techniques%20applied%20to%20traditional,Such%20methods%20are%20effective). In operating systems and low-level software, **profilers** (e.g. `perf`, `gprof`) and advanced techniques like **eBPF** allow developers to monitor performance counters and internal events with minimal overhead, giving deep visibility into running code. Observability in the control-theory sense is also formally defined: a system is *observable* if its full state can be determined from its outputs[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=the%20analysis%20must%20focus%20on,Brockett%2C%202015). For linear systems this relates to the rank of an observability matrix[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=the%20analysis%20must%20focus%20on,Brockett%2C%202015). Recent work by Soatto *et al.* (2024) examines LLMs through this lens, asking whether an LLM’s hidden state trajectory can be inferred from its token outputs[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=For%20LLMs%2C%20lack%20of%20observability,prompts%29%20or%20by%20the)[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=observability%20of%20mental%20state%20trajectories%2C,%E2%80%99%20Our%20contribution%20is%20to). They conclude that current autoregressive Transformers *are* observable in a certain sense (no “hidden” state that doesn’t eventually influence outputs), but modifications to architecture could break this property[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=match%20at%20L566%20observability%20for,tokens%20are%20observable%2C%20but%20simple). While this theoretical perspective is intriguing, most practical work focuses on *operational observability* – monitoring systems to debug and ensure reliability.

**Machine Learning Model Monitoring (MLOps):** Before the era of large language models, the MLOps community had begun addressing model observability in production. Model monitoring tools track metrics like prediction distributions, data drift, and latency. For example, **model drift detection** techniques alert if an ML model’s output distribution shifts over time or if input data no longer resembles training data[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Monitor%20Drift%2C%20Bias%2C%20Performance%2C%20%26,Data%20Integrity%20Issues). However, traditional model monitoring often treats the ML model as a single component to watch (accuracy, input/output logs, etc.), analogous to *monitoring* in DevOps which provides predefined metrics and alerts[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Aspect%20LLM%20Monitoring%20LLM%20Observability,analysis%2C%20optimization%2C%20user%20behavior%20insights). In contrast, **LLM observability** (as defined by recent works) is more holistic – it seeks a *“full context”* view including internal prompts, intermediate steps, and chain-of-thought, not just inputs/outputs[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Aspect%20LLM%20Monitoring%20LLM%20Observability,analysis%2C%20optimization%2C%20user%20behavior%20insights). WitnessAI’s recent guide distinguishes the two: monitoring answers “*what* is wrong” (e.g. latency high, error rate spike) while observability helps with “*why* it breaks and how to fix it” by exposing internal states and fine-grained events[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Aspect%20LLM%20Monitoring%20LLM%20Observability,analysis%2C%20optimization%2C%20user%20behavior%20insights). Our survey specifically focuses on the latter – the new approaches enabling *deep insights* into LLM applications, beyond basic health metrics.

**AI Interpretability and Transparency:** A rich body of work exists on interpretability of neural networks – probing what features or concepts neurons represent, visualizing attention weights, extracting decision rules, etc. While interpretability research is tangentially related (it aims to *explain* model decisions), our survey centers on **debugging and tracing in an operational context**. We cover some interpretability-driven approaches when they are used directly for observability. For instance, **Anthropic’s “circuit-level” interpretability** research instrumented an LLM (Claude) to trace activations during processing, thereby revealing surprising behaviors (like the model secretly planning a rhyme in a poem well before writing it)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,longer%20horizons%20to%20do%20so)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=We%20were%20often%20surprised%20by,and%20often%20have%20been). They term this a “**microscope**” on the model’s computations[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,as%20models%20grow%20more%20sophisticated). Such techniques blur the line between static interpretability and dynamic tracing. Another example is work on **influence functions** and training data attribution: methods like *TrackIn* and *TRAK* (2023) can trace which training examples most influenced a given model output[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=Researchers%20have%20proposed%20a%20variety,2023)[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=promising%20results%20identifying%20examples%20that,2022%3B%20Chang%20%26%20Bergen%2C%202024). These help answer *why* a model stated a particular fact – was it memorized from a source, or hallucinated? – thus aiding *debugging* of knowledge errors. We include these where applicable (e.g. Section 3.2 discusses tracing outputs to training data). However, pure interpretability methods (like mechanistic analysis of model internals that are not aimed at real-time or post-hoc debugging) are outside our scope. The interested reader can refer to surveys on interpretable AI for a broader coverage.

**Multi-Agent System Debugging:** The idea of debugging multi-agent *software* has existed in distributed systems and robotics. Techniques include logging agent messages, using runtime monitors for agent protocols, etc. With LLM-based agents rising, researchers are revisiting these ideas. For example, some works propose *agent-level fault detection* where agents critique each other or themselves (an idea related to self-reflection and chain-of-thought). A notable recent framework is **Watson** by Rombaut *et al.* (2024), which provides *“reasoning observability”* for autonomous LLM agents[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive). Watson introduces a *surrogate agent* that runs in parallel to a primary agent, generating an *approximate reasoning trace* even when the primary agent is a “black-box” fast LLM that does not explain itself[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20then%20introduce%20a%20novel,We)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=3%20Watson%3A%20A%20Framework%20to,powered%20Agents). This recovered trace can then be analyzed to localize errors in the agent’s decision process and even provide hints to correct them at runtime[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20then%20introduce%20a%20novel,We)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=qualitatively%20and%20quantitatively%20evaluated%20the,automatically%20provide%20targeted%20corrections%20at). Such work builds on earlier multi-agent explanation frameworks, but specifically targets modern LLM agents where reasoning is implicit and needs to be extracted. Another related area is **emergent behavior** analysis in multi-agent simulations – e.g. identifying when a group of agents develops unintended strategies. Chaos engineering approaches (discussed later) have been suggested to systematically test multi-agent robustness[arxiv.org](https://arxiv.org/html/2505.03096#:~:text=conditions,Although%20widely%20used%20in%20distributed)[arxiv.org](https://arxiv.org/html/2505.03096#:~:text=vulnerable%20to%20emergent%20errors%20or,reliable%20performance%20in%20critical%20applications).

In summary, while there is prior work in each adjacent area (software observability, ML monitoring, interpretability, multi-agent debugging), the convergence on *observability for LLM and agent systems* is very recent. To our knowledge, no prior survey has focused explicitly on this intersection. Our work aims to fill that gap, synthesizing insights from a wide range of very recent papers and articles (2022–2025) that are often scattered across arXiv preprints, industry whitepapers, and conference workshops. By doing so, we hope to provide researchers and practitioners a coherent understanding of the state-of-the-art in making LLMs and LLM-based agents **transparent** and **debuggable**.

## Technical Dimensions of LLM Observability

Observability in LLM-based systems spans multiple levels of the stack. In this section, we break down the technical approaches into two broad categories: **(1) System-Level Instrumentation and Telemetry** – how we collect and organize data about the operation of LLM systems; and **(2) AI-Level Behavioral Trace and Analysis** – how we capture and analyze the model’s or agent’s behavior (reasoning steps, decisions, errors). In practice, these layers overlap and work in tandem. A robust observability solution requires advances in both: one must instrument the system to emit the right data, and also devise meaningful analyses of that data to debug or improve the AI.

### 3.1 System-Level Instrumentation and Telemetry

**Tracing Architectures for LLM Applications:** Modern LLM applications (for example, a question-answering system using a GPT-4 API, or a LangChain-based agent invoking multiple tools) often resemble microservice workflows, with the twist that **some “services” are LLM calls and prompts** rather than traditional API calls. Recognizing this, several works have adapted *distributed tracing* concepts to LLM applications. The goal is to capture a **trace** of an end-to-end task execution, consisting of spans for each significant step (prompt issuance, model invocation, tool call, response parsing, etc.), along with timing, inputs and outputs, and any errors.

OpenTelemetry (OTel), the open standard for telemetry data, has been leveraged in this context. OpenTelemetry provides a way to instrument code to emit spans and metrics, and to export them to backends like Jaeger or Grafana for analysis[last9.io](https://last9.io/blog/langchain-observability/#:~:text=3)[last9.io](https://last9.io/blog/langchain-observability/#:~:text=endpoint%3Dos.getenv%28,). Several blog articles (Last9, 2023; LangChain blog, 2023) demonstrate tracing a LangChain agent with OTel: each chain or agent action is wrapped as an OTel span, so that a developer can visualize the sequence and nesting of calls[last9.io](https://last9.io/blog/langchain-observability/#:~:text=Add%20Tracing%20to%20FastAPI%20Endpoints)[last9.io](https://last9.io/blog/langchain-observability/#:~:text=def%20on_chain_start%28self%2C%20serialized%2C%20inputs%2C%20,chain_id%29%20except%20Exception%20as%20e). For example, if an agent first calls an LLM to formulate a plan, then makes a web search (tool use), then calls the LLM again to answer, these would be captured as a trace with parent and child spans. The trace can be visualized to show which step took how long and where an error might have occurred. This is analogous to seeing a distributed trace of microservice calls, except here the microservices might be “LLM on OpenAI” or “search tool API.”

A key aspect is correlating all steps of a single user request. Typically, a unique **trace ID** is generated at the start of a request and passed through, even into prompt metadata. If using an LLM API that allows attaching an ID (or otherwise logging one), this can tie together the events. Some frameworks like **LangSmith** (LangChain’s observability platform) provide such end-to-end tracing out-of-the-box, capturing each prompt and model response along with timestamps and success/failure status.

One challenge noted is that using external tracing can sometimes be intrusive or add overhead. Lightweight alternatives include logging traces to JSON files or in-memory structures for later analysis. Open-source tools like **Langtrace** and **Helicone** offer to log each prompt/response with minimal code changes, focusing on capturing **prompt inputs, outputs, token counts, and latencies**. These logs are essentially traces that can be later queried to find patterns (e.g., “find all instances where the agent failed to complete the task”).

**Telemetry Data and Schema:** The raw data needed for observability in LLM systems typically includes: (a) **Prompts and responses** (the full text, or at least identifiers to them for privacy reasons); (b) **Model parameters** like which model, version, hyperparameters (temperature, etc.) were used – since behavior can vary; (c) **Token usage** and costs; (d) **Latency and execution time** for each step; (e) **Tool call details** if an agent uses external tools (which API called, input/outputs, errors); (f) **System metrics** like GPU utilization, memory usage if running on self-hosted models. A structured schema is needed to store this information, especially given the hierarchical nature (one user query -> multiple LLM calls -> possibly sub-calls).

Some initiatives are beginning to propose standard schemas. For instance, OTel’s semantic conventions might be extended for AI – one could imagine a span attribute for “prompt.template” or “model.name”. Indeed, Last9’s guide shows adding attributes like chain IDs and types to spans[last9.io](https://last9.io/blog/langchain-observability/#:~:text=def%20on_chain_start%28self%2C%20serialized%2C%20inputs%2C%20,chain_id%29%20except%20Exception%20as%20e). The goal is to allow uniform analysis – e.g., aggregating average latency per model, or comparing token usage per prompt template version.

Academic proposals like **FMTrace** (hypothetical name) have not fully emerged yet, but a need is identified: a *unified telemetry schema for foundation model interactions*. This would mirror how OpenTelemetry standardized HTTP request telemetry. A challenge is the **privacy** and size of data – logging full prompts and responses can be sensitive and heavy. Some works explicitly mention privacy concerns in LLM observability, since user prompts might contain personal data[ijlrp.com](https://www.ijlrp.com/papers/2024/12/1319.pdf#:~:text=,which%20can%20raise%20significant). Techniques like redacting or hashing parts of prompts, or only logging metadata, are being considered to balance observability with compliance.

**Runtime Introspection and Hooks:** On the more technical side, researchers have explored whether we can instrument *within* the model execution to get deeper observability. For closed-source API models (like OpenAI’s), this is impossible beyond what the API returns. But for open-source models or custom deployments, one can instrument the model at runtime. For example, one could tap into the **forward pass** of the model to dump intermediate representations (activations at certain layers, or probabilities at each step). This is analogous to how **eBPF** can hook kernel functions to record data. A few recent papers have done something similar for **transformer models**: they modify the model to record attention patterns or the hidden states corresponding to each token.

One noteworthy case is the **“interpretability via optimization”** approach by Anthropic (2023), where they introduce small perturbations to activations to test their effect (effectively tracing causal impact of certain neurons on outputs)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=instead%20uses%20a%20different%20planned,for%20this%20entirely%20different%20ending)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=electrical%20or%20magnetic%20currents%29,when%20the%20intended%20outcome%20changes). While this is more for scientific insight, one can imagine a debugger that, say, monitors if any neuron activates beyond a threshold (like a watchpoint in traditional debugging).

Another line of work uses **deterministic replay** hooks. In distributed training of LLMs, tools exist to capture a “trace” of the training process (like sequences of random seeds, batches, etc.) so it can be replayed for debugging. For inference and generation, to replay the exact same sequence of random choices, one must capture the initial random seed and the model state. Some practitioners ensure **seed control** for LLM outputs when debugging, so that errors can be reproduced exactly. However, as one LinkedIn discussion notes, achieving identical outputs across runs or machines isn’t trivial even with fixed seeds[linkedin.com](https://www.linkedin.com/pulse/reproducibility-challenge-large-language-models-practical-kapoor-qhsoc#:~:text=The%20Reproducibility%20Challenge%20in%20Large,parameters%20carefully%20to%20minimize%20randomness). Tiny implementation differences can lead to divergence unless carefully managed.

**Profiling Techniques for LLMs:** Observability isn’t only about logical correctness – performance and resource usage are also critical in LLM systems (which are notoriously heavy). Profiling tools help identify bottlenecks (e.g., a particular model call that is slow or a memory leak across iterations). Traditional profilers (like NVIDIA’s Nsight for GPU, or PyTorch’s autograd profiler) are being used to characterize LLM workloads[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion)[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=2023,offering%20a%20complete%20view%20of). Recent research, such as **Lumos** (2024), built on these profiling traces to model and predict LLM training performance[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=modeling%20and%20estimation%20toolkit%20for,streamline%20the%20exploration%20of%20optimization)[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion). Lumos uses traces from lower-level profilers (PyTorch Kineto, etc.) to construct an execution graph of the model and identify where time is spent[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=Lumos%20is%20the%20first%20system,execution%20graphs%20from%20existing%20traces)[arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion). The fine-grained data (e.g., which operation or communication step consumes how many milliseconds) is analogous to what a sampling profiler gives for CPU programs. This is crucial for *scaling and efficiency debugging*: for instance, if a deployment is too slow, profiling might reveal that the token generation loop is CPU-bound on text processing, or that a certain layer of the model dominates latency.

In the context of multi-agent or complex LLM pipelines, profiling extends to *system-level bottlenecks*: maybe an agent is spending a lot of time waiting on an external tool, or network latency to the model API is high. Observability platforms often include metrics dashboards for these. As one article suggests, it’s important to track not just average latency but outliers, and to correlate token counts with latency to spot when models perhaps output too much text or encounter long loops[medium.com](https://medium.com/@sol.farahmand1986/using-llms-to-evaluate-and-improve-other-llms-in-production-f38685537333#:~:text=Monitor%20Performance%3A%20Track%20latency%2C%20token,Set%20Up).

**Data Logging and Versioning:** A practical aspect is managing all the data collected. With dozens of prompts and responses per user query, and possibly thousands of queries, logs grow quickly. Efficient storage (using vector databases for embeddings, or compression for text) becomes important. Some technical papers (Chen et al., 2023) suggested *schemas for storing chain-of-thought data* to enable later analysis and comparison. In particular, if one is doing evaluation or regression testing of LLM outputs, having logged traces means you can compare a new model version’s chain-of-thought with an old one’s to see differences.

In summary, system-level tools create the **infrastructure for observability**: instrumentation to generate traces, mechanisms to capture metrics and logs, and profilers to measure performance. Next, we turn to what we *do* with this information at the AI behavior level – making sense of it, diagnosing errors, and guiding debugging.

### 3.2 AI-Level Behavioral Tracing and Analysis

On top of the telemetry data, the key challenge is to gain insight into the LLM or agent’s behavior. This involves techniques to extract or reconstruct the model’s reasoning process, to evaluate the quality of its outputs, and to localize the source of any issues. We discuss approaches in prompt tracing, agent transparency, error attribution, and taxonomy of reasoning steps.

**Prompt Tracing and Chain-of-Thought:** One straightforward way to make an LLM’s reasoning visible is to prompt it to *“think out loud.”* **Chain-of-Thought (CoT) prompting** (Wei et al., 2022) gets the model to produce intermediate reasoning steps in natural language before giving a final answer. This has a dual benefit: it can improve performance on complex tasks, and it provides a trace of the model’s thought process. Many LLM-based agents now use CoT or a variant (sometimes called *ReAct*, which intermixes reasoning and actions). Logging these chains-of-thought yields rich observability data: a step-by-step narrative of what the model is considering. For example, an agent’s CoT might show: “Step 1: I need to find X. Step 2: I will use the search tool…” etc., which helps a developer follow the logic.

However, several works warn that chain-of-thought traces **may not always reflect the true internal reasoning** of the model[bdtechtalks.substack.com](https://bdtechtalks.substack.com/p/llms-reasoning-traces-can-be-misleading#:~:text=LLMs%20reasoning%20traces%20can%20be,actually%20arrived%20at%20its%20answer)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,its%20fake%20reasoning%2C%20providing%20a). An LLM might generate a plausible-sounding explanation that hides a fundamentally flawed or different reasoning. As an illustrative case, Anthropic (2023) demonstrated that their model *Claude* sometimes produces a chain-of-thought that is basically *rationalization*: if the user provides a misleading hint, Claude will generate reasoning that *agrees* with the hint rather than logically solving the problem[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,its%20fake%20reasoning%2C%20providing%20a). They “caught it in the act” – the model’s reasoning steps showed it making up a fake logical argument just to align with the user’s wrong suggestion[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,its%20fake%20reasoning%2C%20providing%20a). This highlights that naive prompt-based traces cannot be taken as ground truth of model cognition. Nevertheless, CoT remains a valuable tool for debugging: it at least surfaces something about what the model *might* be doing, and anomalies in CoT (like leaps of logic or contradictions) can signal where things go wrong.

**Implicit Reasoning and Post-hoc Extraction:** What about models or agents that do not naturally output their chain-of-thought (either by design or due to efficiency constraints)? This scenario is common in deployed systems where “fast-thinking” LLMs are used without verbose reasoning to save latency[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Although%20recent%20Large%20Reasoning%20Models,For%20example%2C%20the%20code%20auto). Researchers have started developing methods to recover the *implicit* reasoning after the fact. The **Watson framework (2024)** is a prime example[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive). In Watson, alongside the primary agent (an LLM that solves tasks without explaining itself), a secondary process uses a *surrogate LLM* to generate a reasoning trace that *could have led to the observed output*[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20then%20introduce%20a%20novel,We). Importantly, Watson aims to not disturb the primary agent – it observes inputs and outputs and then uses a combination of prompt-based techniques (including a fill-in-the-middle generation, where the surrogate is asked to fill in a reasoning that would connect the input to the output) to produce a hypothetical chain-of-thought[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Watson%20is%20structured%20around%20three,primary%20agent%2C%20allowing%20for%20the)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Generating%20Reasoning%20Paths%3A%20With%20the,the%20completion%20of%20a%20primary). They validate that this reconstructed reasoning is often accurate to what a truthful CoT would be, by using prompt attribution techniques[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20then%20introduce%20a%20novel,We). Once the reasoning is obtained, Watson can analyze it to find errors in the agent’s logic. In their experiments, they used Watson to debug an agent’s poor performance on knowledge tasks (MMLU benchmark) – for instance, if the agent answered a question incorrectly, Watson’s trace could highlight that the agent misunderstood a term, allowing an automated hint to be generated to correct it[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20perform%20qualitative%20evaluation%20by,agent%20to%20make%20the%20correct)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=match%20at%20L439%20primary%20agent,to%20generate%20a%20hint%20that). This improved the agent’s accuracy significantly when the hints were fed back in[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=reasoning%20observability%20into%20the%20implicit,relative%20improvement%29%20and).

Other approaches to extract reasoning include: running the model in a *step-by-step execution mode* (if supported), or training a separate model to predict an agent’s likely next thought (a kind of observer model). There is also research on using **language model self-critiquing**: the model is asked after producing an answer, “Please reflect if this answer might be wrong and why.” This isn’t exactly tracing the original reasoning, but it leverages the model to analyze its own output. While not foolproof, self-critiques have been shown to catch a portion of errors (like mathematical mistakes or factual hallucinations) by essentially asking the model to simulate a *debugger persona*.

**Observability Metrics for LLM Outputs:** In classical software, one monitors metrics like error rates, latency, memory usage. For LLMs, we need **semantic metrics** as well – indicators of output quality and behavior. Several papers and tools propose metrics such as:

- **Truthfulness/Factuality:** e.g., the rate of factual errors or hallucinations. Techniques to measure this include automated fact-checkers or knowledge-base queries. Arize’s LLM observability solution, for instance, can flag likely hallucinations by checking if the LLM’s answer contains entities or statements that were not present in the retrieved context or contradict known references[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=,OpenAI%2C%20Anthropic).
- **Toxicity or Bias Scores:** using models like Perspective API or toxicity classifiers to assign a score to each output for hate, profanity, etc.[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=responses.%20,LangChain%2C%20LlamaIndex%2C%20and%20Python%20SDKs). These metrics help monitor if an agent’s behavior stays within safe bounds over time. Unusual spikes in toxicity scores might indicate a prompt injection attack succeeded in manipulating the model into bad behavior.
- **Helpfulness/Quality Ratings:** sometimes collected via user feedback or by an automatic LLM-as-a-judge system. For example, one could periodically use a GPT-4 to rate the helpfulness of responses from a deployed smaller model (akin to an automated QA process).
- **Consistency and Stability:** metrics that capture if the model’s answers are self-consistent. One approach (from evals like *self-consistency decoding, 2022*) is to sample multiple outputs for the same prompt and see if they converge on the same answer. High variance might indicate uncertainty or instability in responses.
- **“Agent span” metrics:** If we define categories for an agent’s actions (e.g. “Planning step”, “Tool use: search”, “Tool use: calculation”, “Final answer”), we can count and measure each. For instance, we might find an agent invokes the calculator tool on 30% of queries – which might correlate with certain errors. A taxonomy of agent spans allows structured analysis. Although formal taxonomies are still emerging, some have begun to label agent traces in this way, often inspired by the **OpenTelemetry span model**. In OpenTelemetry, each span has a name and attributes; analogously, one can label each reasoning step in a CoT with a category (question analysis, gathering info, etc.). This area is ripe for standardization – an “Agent Observability Schema” could define a set of common span types (like *PROMPT_STEP*, *MODEL_INFERENCE*, *TOOL_CALL*, *ANSWER*) for any LLM-based agent trace.

The above metrics can be used for **alerting** and dashboards. For example, an observability system might have an alert if hallucination rate goes above X%, or if average answer length drops suddenly (could indicate the model is cutting off answers due to some hidden constraint). In practice, these metrics often require comparing model outputs against references or using other models, which can be expensive; hence they might be computed on a sample of traffic or offline.

**Assertion-Based Debugging and Guardrails:** In software debugging, developers often use assertions – conditions that must hold true at a certain point in execution, otherwise an error is raised. Analogously, researchers are adding **guardrails** or **constraints** to LLM outputs to catch errors. Recent work by Lee *et al.* (2025) introduces *Semantic Integrity Constraints (SIC)* in AI-augmented database systems[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=AI,with%20both%20proactive%20and%20reactive)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=LLM%20constraints%E2%80%94also%20known%20as%20assertions,A%20related%20approach%20is%20constrained). These are basically *declarative guardrails* that specify correctness conditions (e.g., “the SQL query explanation should always match the given query”). If the LLM’s output violates a constraint (checked via a user-defined function or validator), the system can reject or fix it[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=limiting%20their%20adoption%20in%20critical,proactive%20and%20reactive%20enforcement%20strategies)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=LLM%20constraints%E2%80%94also%20known%20as%20assertions,A%20related%20approach%20is%20constrained). Lee *et al.* point out that such constraints generalize the notion of database integrity constraints to LLM outputs, and they can enforce things like grounding (the answer should only use provided context) or non-deviation (the summary must not introduce new facts)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=limiting%20their%20adoption%20in%20critical,proactive%20and%20reactive%20enforcement%20strategies)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=2024a%20%2C%20%2026%3B%20Dong,2024).

These guardrails effectively serve as **runtime assertions** for LLMs. They improve observability because they make errors explicit: rather than a subtle hallucination slipping through, a guardrail can flag it (fail an assertion) so it is observed and handled. There are industry tools (e.g. **GuardrailsAI**, 2023) that let developers specify such rules in a simple way, for instance: output must be JSON parseable, or must mention certain keywords if the input did, etc. When integrated with an agent, these can prevent the agent from proceeding on faulty data (much like an assertion aborts a program to avoid propagating corruption).

A limitation is that not all desired properties can be easily formalized as constraints. Nonetheless, common failure modes (like output format errors, missing citations, toxic language) can be addressed. GuardAgent (2023) is another recent approach that wraps an agent with a *watchdog agent* looking for safety rule violations[invariantlabs.ai](https://invariantlabs.ai/guardrails#:~:text=Introducing%20Guardrails%3A%20The%20contextual%20security,to%20augment%20existing%20agentic%20models)[medium.com](https://medium.com/@ben.burtenshaw/taming-toxic-language-with-prompt-engineering-and-nvidia-nemo-guardrails-536789f4c6b6#:~:text=In%20this%20article%2C%20I%20will,of%20setting%20up%20predefined%20responses) – similar in spirit to assertions but implemented via a second model.

**Error Attribution and Root Cause Analysis:** When an LLM-based system produces an incorrect or harmful result, observability tools seek to trace back to the cause. Possible causes include: a bad prompt (ambiguous or leading), a model limitation (the model lacks knowledge or skills), a tool failure (the agent got bad info from a tool), or a coding error in the agent logic. Determining which it is can be challenging. Some strategies from the literature:

- **Influence tracing to training data:** As mentioned, methods like *TrackStar* (Chang et al., 2024) attempt to find which training examples most influenced the model’s output[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=promising%20results%20identifying%20examples%20that,2022%3B%20Chang%20%26%20Bergen%2C%202024)[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=Researchers%20have%20proposed%20a%20variety,2023). If a particular erroneous fact keeps appearing, influence tracing might show it came from a specific source in training. If it’s a case of misinformation, one could then fix the data or prompt the model to unlearn that fact (at least temporarily via context).
- **Intermediate state analysis:** For chain-of-thought agents, one can inspect the intermediate steps. For example, if the final answer was wrong, maybe step 3 of the reasoning was factually wrong – that’s the true point of failure. Watson (2024) essentially does this: it identifies the step in the implicit reasoning where the agent took a wrong turn[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=reasoning%20observability%20into%20the%20implicit,relative%20improvement%29%20and). Another approach is to use *step-by-step test cases*. LDB (2024) is a system for debugging code generated by LLMs by executing the code and checking each step against expected behavior[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=program%2C%20they%20do%20more%20than,2001)[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=visible%20test%20case%2C%20LDB%20segments,the%20quality%20of%20code%20generation). While LDB is specific to code, the principle extends: run the agent through a known scenario and see where it deviates. LDB’s authors note that human debuggers don’t just look at final output, they *observe the runtime execution trace and intermediate state*, setting breakpoints – and they bring that paradigm to LLM-generated programs[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=program%2C%20they%20do%20more%20than,2001)[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=visible%20test%20case%2C%20LDB%20segments,the%20quality%20of%20code%20generation). For agents, one could similarly set “breakpoints” in the chain-of-thought – e.g., after the agent formulates a plan, inspect that plan before letting it execute further.
- **Attributing blame in multi-component pipelines:** Many applications combine an LLM with other components (retrievers, databases, user inputs). If the final answer was wrong, was it because the retrieval fetched irrelevant documents or because the LLM mis-summarized correct documents? Observability tooling often tries to log each component’s output. Arize Phoenix, for instance, lets one slice and dice data: you could filter to all cases where the retrieval confidence was high but answer was wrong, versus retrieval was low confidence. This helps isolate if the retriever or the generator is the bottleneck. Some research has proposed coupling *uncertainty estimation* with LLMs – for example, the agent might carry a confidence score that can be logged and analyzed. If the model was actually very unsure (low confidence) when it made a mistake, it might indicate just a knowledge gap, whereas high confidence + wrong answer is more concerning (the model “thought” it was right).
- **Span-level attribution in traces:** If using distributed tracing analogies, one could attach error tags to spans. For example, if a tool returns an error (API 500), the trace span for that call can have an error flag. Later analysis could show that many failures happened due to tool errors vs. model errors.

In complex multi-agent settings, error attribution might involve analyzing *inter-agent communication*. If agents are chatting to coordinate and something fails, was it due to a misunderstanding between them? Observability can log all messages (with agents’ internal states if available). Some theoretical work suggests creating **communication taxonomies** to identify types of messages (e.g. assertive vs interrogative communications among agents) and pinpoint when an agent gave misleading info to another.

**Visualization and Developer Tools:** Making sense of collected data often requires visualization. Several tools have introduced **trace viewers** specifically for LLM apps. These display a tree of calls, showing how the chain progressed. LangSmith’s trace viewer, for instance, shows each prompt template filled with actual values and the LLM’s response, indenting nested calls (like a tool call within an agent) – this is essentially a **debugger UI** for prompt flow. It helps a developer answer questions like: Did the agent follow the expected chain? Did it call the calculator when it should have?

Another important tool is the **token analyzer or spotlight**. Since LLM output is token by token, some debuggers let you step through generation. For example, Huggingface’s **VisualizeGPT** (an experimental tool) shows at each token what the model’s top choices were. This can highlight if a certain token choice caused divergence. It’s analogous to stepping through code line by line.

We also see **comparison tools** emerging – for instance, to debug regressions, a tool might show two traces side by side (old vs new model) highlighting differences in chain-of-thought or output. This is extremely useful in identifying *which step a new model started to go wrong relative to an older one*.

**Parallels to Classical Debugging:** To summarize this section, Table 1 draws parallels between classical software debugging tools and the LLM observability techniques we’ve discussed:

| Classical Concept | LLM/AI Analog | References |
| --- | --- | --- |
| Logging (print statements) | Prompt/response logging, chain-of-thought logging | [witness.ai](https://witness.ai/blog/llm-observability/#:~:text=%2A%20Real,integration%3A%20Support%20for%20tools%20like)[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Monitor%20Drift%2C%20Bias%2C%20Performance%2C%20%26,Data%20Integrity%20Issues) |
| Breakpoints & Step-through | Manual checkpoints in chain-of-thought, step-by-step prompting, or pausing agent loops | [arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=program%2C%20they%20do%20more%20than,2001)[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=visible%20test%20case%2C%20LDB%20segments,the%20quality%20of%20code%20generation) |
| Assertions | Guardrails and semantic constraints on outputs | [arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=LLM%20constraints%E2%80%94also%20known%20as%20assertions,A%20related%20approach%20is%20constrained)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=2024a%20%2C%20%2026%3B%20Dong,2024) |
| Stack Trace | Trace of nested tool calls and LLM invocations | [last9.io](https://last9.io/blog/langchain-observability/#:~:text=Add%20Tracing%20to%20FastAPI%20Endpoints)[last9.io](https://last9.io/blog/langchain-observability/#:~:text=def%20on_chain_start%28self%2C%20serialized%2C%20inputs%2C%20,chain_id%29%20except%20Exception%20as%20e) |
| Distributed trace (span) | Agent span taxonomy (plan, tool-call, answer span, etc.) | (Section 3.2 text) |
| Profiler (CPU/GPU) | Model performance profiler (token throughput, latency per step) | [arxiv.org](https://arxiv.org/pdf/2504.09307?#:~:text=As%20the%20ML%20system%20stack,metrics%20and%20enabling%20effective%20optimiza%02tion) |
| Unit tests | Automated queries to test model on specific cases (with expected chain-of-thought) | (conceptual, e.g. LDB style[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=program%2C%20they%20do%20more%20than,2001)) |
| Reproducible replay | Fixed random seeds, logging RNG state to replay generation | (discussed concept; LinkedIn[linkedin.com](https://www.linkedin.com/pulse/reproducibility-challenge-large-language-models-practical-kapoor-qhsoc#:~:text=The%20Reproducibility%20Challenge%20in%20Large,parameters%20carefully%20to%20minimize%20randomness)) |
| Monitoring dashboards | Live dashboards for LLM usage (token counts, error rates, drift metrics) | [witness.ai](https://witness.ai/blog/llm-observability/#:~:text=A%20robust%20observability%20solution%20for,LLMs%20should%20provide)[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Monitor%20Drift%2C%20Bias%2C%20Performance%2C%20%26,Data%20Integrity%20Issues) |

This mapping illustrates how many traditional debugging ideas are finding a counterpart in the LLM world. Yet, there are fundamental differences – notably, the *nondeterministic and probabilistic* nature of LLMs, and the fact that their “internal state” (the billions of weights) is not human-readable. Thus, while we can borrow concepts, new innovations are needed to truly address the AI-specific challenges, as we discuss next in Section 4.

## Challenges

Our survey of the literature reveals several overarching challenges that researchers and practitioners face in observability and debugging of LLM-based systems. We summarize the key challenges below, along with insights from recent work on each.

**C1: Opaque Reasoning and Lack of Ground Truth for Thoughts.** Unlike traditional software where the internal state (variables, stack, etc.) is well-defined and can be inspected, LLMs reason in a latent space that is not directly interpretable. Even if we extract a chain-of-thought, there is no guarantee it reflects the true internal computation. *Observability of latent state* in LLMs is fundamentally hard – as Soatto *et al.* note, one can ask if there exist distinct “mental state trajectories” that produce the same outputs (if yes, the system is unobservable in that aspect)[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=For%20LLMs%2C%20lack%20of%20observability,prompts%29%20or%20by%20the). They argue that current LLMs *are* observable to the extent that any hidden chain-of-thought inevitably influences outputs eventually[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=match%20at%20L566%20observability%20for,tokens%20are%20observable%2C%20but%20simple). However, practically, distinguishing whether an answer was arrived at via correct reasoning or by a lucky guess remains difficult if the model doesn’t show its work. There is *no ground truth for the model’s reasoning* to compare against. This challenge is analogous to testing nondeterministic or heuristic programs where multiple internal paths yield the same result – you can test output accuracy, but verifying the “right process” was followed is harder. Recent “Large Reasoning Models” that explicitly output their reasoning provide one possible solution[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=Although%20recent%20Large%20Reasoning%20Models,For%20example%2C%20the%20code%20auto), but at the cost of speed and cost. Research like Watson (2024) attempts to recover hidden reasoning, but even Watson’s surrogate reasoning is an approximation[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=We%20then%20introduce%20a%20novel,We). Addressing this challenge likely requires advances in *mechanistic interpretability* and *validated reasoning traces* – perhaps developing models that not only output reasoning but also *guarantee* certain logical steps were taken (an open research problem).

**C2: Non-determinism and Reproducibility.** LLMs introduce randomness in their outputs (except in degenerate cases of temperature 0 and certain decoding). This means a bug might not manifest on every run, or a carefully logged trace might not repeat. It’s reminiscent of debugging race conditions or concurrency issues in software. As noted in Hassan *et al.*’s challenges, testing under non-determinism is tricky – generative tasks don’t have a single expected outcome[aiwarebootcamp.io](https://www.aiwarebootcamp.io/slides/2024_aiwarebootcamp_hassan_rethinking_software_and_se_in_the_fm_era.pdf#:~:text=Testing%20under%20non,tasks%20has%20no%20ground%20truth). One cannot simply unit-test an LLM response against a fixed expected string (unless it’s a very constrained task). This non-determinism complicates the observability: you might capture a problematic output, but on replay the model produces a fine answer, making the issue hard to pin down. Some practitioners attempt to control this by using fixed random seeds when possible, or analyzing distributions of outputs rather than one output. The LinkedIn discussion[linkedin.com](https://www.linkedin.com/pulse/reproducibility-challenge-large-language-models-practical-kapoor-qhsoc#:~:text=To%20achieve%20deterministic%20model%20outputs,parameters%20carefully%20to%20minimize%20randomness) mentions careful parameter choices to minimize randomness, but even then factors like multi-threading on GPU can lead to differences. There is a need for **deterministic replay mechanisms** for LLMs – perhaps a way to log the random generator state along with prompts. For API-based models, this would require support from providers (e.g., an “execution ID” that can be re-run, which currently doesn’t exist). Without reproducibility, debugging remains a stochastic game of probabilities. Researchers might need to borrow ideas from fuzz testing or statistical debugging: run the LLM many times and use observability data to infer what internal conditions correlate with failures.

**C3: Performance Overhead vs. Visibility Trade-off.** More observability typically means more data collection, which can slow systems or overwhelm with data. For LLM applications, logging everything (every token’s probability, every intermediate thought) is not feasible at scale. There is a challenge in *selective observability*: capturing enough information to be useful without incurring too much cost or latency. Techniques like sampling traces (only logging a fraction of queries) or *dynamically toggling observability* when an anomaly is detected are being considered. For instance, one could run an agent normally (fast, no verbose output) but if it gets a question very wrong (as detected by a quick heuristic), then re-run it with chain-of-thought logging enabled to get a trace for debugging. This “on-demand observability” is analogous to how some systems turn on verbose logging when errors occur.

Another aspect is storage/processing of large traces – organizations report that naive logging of LLM interactions can produce massive data (every prompt might be several KB of text, and responses even more). Efficient encoding (like storing only diffs, or vectorizing for search) is needed. Challenge is to do this without losing the human-readable context that developers need.

**C4: Integration of Multiple Data Sources (Telemetry Fusion).** In a complex system, different tools might capture different pieces of observability data: application logs, model API logs, user feedback, etc. Merging these into a coherent picture is non-trivial. For example, a user says “the answer was irrelevant”. One might need to pull the trace of that request, see that the retrieval component returned documents about a wrong topic – so maybe label it a retrieval error. But if those systems aren’t integrated, it’s difficult. Efforts like the standardized schema aim to address this, but currently many solutions are siloed. A unified observability platform for LLMs that can ingest **traces, metrics, AND semantic evaluations** is still an open need. There’s also the challenge of correlating issues across layers: e.g., noticing that a spike in latency correlates with a spike in hallucinations (perhaps because a slow external tool caused the agent to “improvise” an answer). Such correlations require cross-layer analysis that few tools currently provide out-of-the-box.

**C5: Defining “Failure” and Ground Truth in Open-Ended Tasks.** In traditional software, a failure might be an exception, a wrong return value, or not meeting a spec. In LLM outputs, especially open-ended ones, failure is often subjective. Is an answer “good enough”? Was a story “on topic”? This makes automated observability hard – you need some oracle or evaluation metric. Some works use reference datasets or automated metrics (e.g., BLEU, ROUGE for summaries, or human preference models), but these are proxies. The lack of a clear ground truth means observability is often about *detecting anomalies* or *policy violations* (e.g., hate speech) rather than correctness. Human-in-the-loop is still needed for final judgment in many cases. So, one challenge is to improve **automatic evaluation** methods so they can be used in observability. HELM (Holistic Evaluation of Language Models, 2022) introduced a broad set of metrics for various aspects (accuracy, robustness, fairness, etc.), and one could imagine an *observability dashboard* showing HELM metrics over time for a deployed model. But computing many of those metrics requires labeled data or expensive procedures.

**C6: Security and Privacy Concerns.** Observability data can itself pose risks. Logging user queries and model outputs verbatim raises privacy issues[ijlrp.com](https://www.ijlrp.com/papers/2024/12/1319.pdf#:~:text=,which%20can%20raise%20significant). If an agent is operating in a sensitive domain (medical, legal), its traces may contain confidential information. Storing and handling this data must comply with regulations (GDPR, HIPAA, etc.). Moreover, the observability mechanisms could be targets: e.g., a malicious user might exploit the logging by injecting very large prompts (causing log overload) or sensitive content (to create liability in logs). Additionally, model providers might not allow certain internal info to be exposed (if one is using a closed API, you can’t instrument it without possibly violating terms). The community needs approaches like *privacy-preserving logging* – perhaps automatic redaction of PII, or only logging aggregate features (embedding vectors) instead of raw text (though those can be inversely transformed in some cases). Homomorphic hashing of prompts (where you log a hash that allows detection of repeats but not reconstruction) could be another idea.

On the security side, observability can help detect attacks (like prompt injections as anomaly in prompt tokens[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=,OpenAI%2C%20Anthropic)), but the observability channel itself must be secure (an attacker could try to hide their traces or flood the monitoring with false alarms). Ensuring integrity of trace data is important if it’s used for auditing.

**C7: Evolving Models and Data – Drift and Versioning.** As LLM-based systems evolve (models get fine-tuned or updated), observability has to reset baselines. A big challenge is **comparing across versions** and managing *regressions*. A certain behavior might have been fine in v1, and in v2 it becomes problematic. Observability solutions should ideally highlight “new errors not seen before” or “significant changes in metrics”. Some papers discuss *concept drift* for LLMs – where over time, maybe due to fine-tuning on user data, a model’s style or biases shift[evidentlyai.com](https://www.evidentlyai.com/ml-in-production/data-drift#:~:text=AI%20www,learning%20model%20is%20in%20production). Detecting this requires long-term analysis and possibly *continuous evaluation pipelines*. There’s a need for research into **baseline characterization** of models: knowing what is “normal” for a given model so that deviations stand out. In complex multi-agent systems, even adding a new agent or tool is like a new version of the system – you’d want to know how it impacts overall behavior (for example, does adding a “Critic agent” to debate answers actually reduce hallucinations as hoped, or does it introduce more delays and confusion? Only careful observation will tell).

**C8: Human Interpretability of Observability Data.** Ironically, making a system observable can flood developers with information that is itself hard to interpret. Traces with hundreds of steps, massive logs, and numerous metrics dashboards can overwhelm. A challenge is providing **summaries** and **explanations** of the observability data itself. We might need meta-tools: e.g., an LLM that reads the logs and explains “The agent often fails when dealing with date calculations – likely an off-by-one error in logic.” Some early work hints at this: using LLMs to analyze other LLMs’ traces[medium.com](https://medium.com/@prabhavithreddy/controlling-creativity-how-to-get-reproducible-outcomes-from-llms-016ec0991891#:~:text=Controlling%20Creativity%3A%20How%20to%20Get,fixed%20value%20before%20generating%20text) or to suggest debug fixes[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=tests%20and%20program%20verifiers%20into,a%20novel%20debugging%20framework). Visualization techniques also need to adapt – maybe highlight just the differences between a good trace and a bad trace, or cluster similar errors together.

In debugging multi-agent interactions, one might need to highlight just the relevant subset of communications leading to a failure, akin to how a debugger might hide irrelevant stack frames. There’s an opportunity to use AI to help analyze AI logs (closing the loop, albeit carefully to avoid misanalysis).

**C9: Standardization and Best Practices are Lacking.** Finally, a non-technical but important challenge is the lack of standards and shared best practices. Every organization is currently inventing its own logging format, its own set of metrics of interest, and often not publishing details due to proprietary concerns. This slows collective progress. The situation is reminiscent of early days of software logging where every app had different log formats until things like syslog and structured logging became common. Similarly, without common observability frameworks, comparing results from different research or tools is hard – one might report a “debugging success” on some task, but how to generalize it is unclear. The community would benefit from benchmarks for *debuggability*: e.g., standardized scenarios where an agent fails in known ways and the task is to see if an observability method catches it. Some initial benchmarks like *Humaneval-X (with chain-of-thought)* exist for code debugging by LLMs, but more are needed across domains.

To summarize the challenges: LLM observability sits at the intersection of hard AI problems (transparency of deep models) and hard systems problems (monitoring complex, nondeterministic distributed processes). The works we surveyed have made tangible progress on specific aspects, but many of the above challenges remain only partially addressed. In the next section, we look ahead to how future research might overcome these hurdles.

## Future Work and Research Directions

The fast-evolving nature of LLM-based systems means observability techniques must continuously adapt. Based on the surveyed literature and the identified challenges, we outline several promising directions for future research and development:

**F1: *Cognitive Trace Logging* – Merging Mechanistic Interpretability with Runtime Tracing.** One vision is to create systems that log not only the external events (prompts, outputs), but also a **meaningful representation of the model’s internal state** at each step. For example, future language models might expose a subset of their activations or attention weights in an interpretable format (such as natural language descriptors of what each neuron cluster represents). Projects like Anthropic’s work on “superposition” and feature visualization[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=it%20only%20answers%20questions%20when,as%20models%20grow%20more%20sophisticated)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=Recent%20research%20on%20smaller%20models,compared%20to%20a%20smaller%20model) hint that models have internal units corresponding to human-understandable features. If those could be monitored (e.g., a model could tell us “I’m currently thinking about topic X with sentiment Y”), it would revolutionize observability. One concrete step is developing *instrumented LLMs* – versions of models that have extra “sensors” in their architecture to output traces of reasoning. Google’s recent *“Thought Cloning”* research (2023) is an example where models were trained to produce their chain-of-thought as they solve problems. A future model might produce a machine-readable reasoning graph as well, which could be consumed by monitoring tools.

**F2: Standardized Observability Interface for AI (OpenTelemetry-AI).** An initiative could be launched to extend standards like OpenTelemetry to cover common AI scenarios. This would involve defining trace/span schemas for prompts, model inference, tool usage, etc., and metric schemas for things like token counts and model confidence. If widely adopted, this would enable plug-and-play observability: any compliant LLM framework would emit events that can be understood by any OTel backend. Researchers and companies could collaborate on an **open-source collection of instrumentation** for popular frameworks (LangChain, Transformers, etc.) so that developers don’t have to reinvent logging for each project. The standard should also consider *privacy* (maybe an option to anonymize or not log certain data by default). There is movement in this direction (some early blog proposals exist[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=1,tools%20support%20root%20cause%20analysis)[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=A%20robust%20observability%20solution%20for,LLMs%20should%20provide)), but formal standardization through groups like the OpenTelemetry community or MLCommons would accelerate it.

**F3: Advanced Automated Debugging Tools (AI Debuggers).** We foresee more tools where *one AI helps debug another*. For instance, imagine a “AI Debugger Agent” that can ingest the logs/traces of a faulty agent and automatically suggest a fix or even apply a patch (like adjusting a prompt or fine-tuning on a counterexample). Early prototypes of this appear in research: e.g., LLMs used to self-correct code they wrote by analyzing runtime errors[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=tests%20and%20program%20verifiers%20into,a%20novel%20debugging%20framework). Extending that, an AI debugger could watch an agent conversation and interject when it sees an error pattern (“It looks like both agents are stuck in a loop, I will inject a new prompt to break it.”). This borders on the agents having meta-cognitive abilities. One concrete idea: train a model on a corpus of known agent failures and resolutions, so it learns to recognize common failure modes (for example, “the agent keeps apologizing and not progressing – likely it got confused by user’s last request; suggest resetting context”). The **evaluation** of such AI debuggers would be important – essentially, we’d benchtest how much they improve reliability when plugged in.

**F4: *Deterministic Simulation Environments* for LLM Agents.** Borrowing from software testing, there’s an opportunity to create sandbox environments where LLM agents can be run with controlled randomness for debugging. For example, a simulator could intercept calls to the language model and use a fixed seed or a small finite state model to mimic the LLM for specific scenarios. While the real model is complex, for testing certain logic one could replace it with a dummy that has deterministic behavior (like always output a certain pattern for given inputs). This is analogous to using mocks in software testing. Developing **agent simulators** that can systematically exercise agent workflows (including environment responses) would help in observing how agents behave in edge cases. Some work in multi-agent simulations (e.g., virtual environments for agents to play roles) could be extended for testing: intentionally introduce a faulty tool response and see if the agent handles it, etc. Observability in these simulations can be cranked to maximum since privacy isn’t a concern – this might produce data to improve the agent before real deployment.

**F5: Integration of Formal Methods and Verification.** An ambitious but potentially high-impact area is bringing formal verification into LLM agent behavior. If certain critical properties can be specified (say, “the agent should never reveal key X” or “if user provides a number N, the final answer must contain a number”), then techniques like model checking or program verification could be applied to the agent’s decision process (which might be seen as a program with probabilistic branches). Some recent works attempt to create *logical specifications* of what an agent does. For example, one could use temporal logic to specify an agent’s dialogue constraints and then monitor compliance at runtime (this overlaps with guardrails). Formal methods could complement observability by not just observing but *proving* certain properties hold in all observed runs. While verifying neural network behavior is extremely hard in general, a limited scope (like verifying a constrained chain-of-thought adherence to rules) might be possible.

**F6: Cross-domain and Multi-modal Observability.** Many LLM systems will be integrated with vision (images) or robotics (actions in the physical world). Observability will need to extend to those: e.g., tracing not just text but also an agent’s visual perception and action decisions. Future research might look at an agent that, say, sees an image and describes what it’s doing in its chain-of-thought (some initial work with “voyageur”-type agents do text-only CoT for code with vision). Ensuring transparency in multi-modal models (like GPT-4’s vision features) is even more challenging – one can’t easily log what the model “thought” about an image unless it explains it. Perhaps an observability approach is to pair the model with a **narrator** that describes its visual processing (e.g., “the model seems to focus on the upper left of the image where a face is”). This is speculative, but multi-modal and multi-agent (swarms of agents) scenarios will stress-test current observability methods. Research into summarizing complex interactions (imagine 10 agents collaborating – how to present that trace succinctly?) will be needed.

**F7: User-Facing Transparency Tools.** Thus far we focused on developers debugging systems. Another dimension is end-users might benefit from some observability of AI. For example, a user might see a “Why did the AI say this?” button that, behind the scenes, triggers an explanation based on the trace or an on-demand reasoning. Some papers on *explanatory debugging* (Kulesza et al., 2015) explored how users can correct models by seeing explanations. Bringing observability to the user can build trust and also crowdsource debugging: users might spot an issue in the chain-of-thought and report it. Of course, care must be taken as the chain-of-thought might contain raw or sensitive info that shouldn’t be shown. Perhaps future UI could show a sanitized or conceptual trace to the user.

**F8: Benchmarking and Shared Datasets.** We encourage the creation of shared benchmarking tasks focused on observability and debuggability. For instance, a challenge where teams are given a flawed LLM agent and must use observability tools to identify the bug and fix it, measuring time or accuracy of diagnosis. Or datasets of model traces labeled with where the reasoning went wrong, to train AI debuggers. Initiatives like the *ML Sys Ops* benchmark (hypothetical) could drive progress by providing common ground to evaluate different approaches (similar to how ImageNet drove vision). Some beginnings might be found in the *CRITIC* and *REFLEXION* papers (2023) where agents improve via self-reflection – those could be turned into a debugging competition.

In conclusion, the future of LLM observability and debugging will likely be a synergy of **software engineering, machine learning, and even cognitive science**. We may find ourselves developing *“introspection sub-systems”* for AI – analogous to a mind that can observe its own thoughts. Achieving this will not only help developers but also ensure AI systems are safer and more aligned with our intentions, as transparency is a prerequisite for accountability[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Observability%20complements%20monitoring%20by%20enabling,world%20conditions). By pursuing the directions above, researchers can bring us closer to AI systems that are as debuggable and trustworthy as the traditional software systems we have today.

## Conclusion

Observability and debugging techniques are the unsung enablers of safe and reliable software. As we integrate large language models and AI agents into critical applications, the ability to inspect, trace, and correct their behavior becomes paramount. This survey has chronicled the nascent but rapidly growing efforts to shine light into the “black box” of LLMs – from system-level telemetry frameworks that record each prompt and response, to AI-level methods that extract an agent’s hidden reasoning. We have drawn parallels with classic systems (logs, traces, breakpoints, profilers) while highlighting novel challenges posed by AI’s opaqueness and unpredictability.

The literature from 2022–2025 reflects a community recognizing that **“intelligence” without transparency is inherently risky**. Encouragingly, solutions are emerging:  techniques like chain-of-thought prompting, if used carefully, give a window into model thinking; frameworks like Watson demonstrate that even implicit reasoning can be partially observed and corrected[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=reasoning%20observability%20into%20the%20implicit,relative%20improvement%29%20and); assertion-like guardrails catch many failure modes at runtime[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=LLM%20constraints%E2%80%94also%20known%20as%20assertions,A%20related%20approach%20is%20constrained)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=2024a%20%2C%20%2026%3B%20Dong,2024); and analogies to observability in distributed systems (like OpenTelemetry traces) are proving powerful in structuring LLM application logs[last9.io](https://last9.io/blog/langchain-observability/#:~:text=Add%20Tracing%20to%20FastAPI%20Endpoints)[last9.io](https://last9.io/blog/langchain-observability/#:~:text=def%20on_chain_start%28self%2C%20serialized%2C%20inputs%2C%20,chain_id%29%20except%20Exception%20as%20e). We also saw that bridging the gap between **human mental models and AI decision processes** is a recurring theme – whether through surrogate models that explain another model, or through visualizations that allow a person to step through an AI’s steps.

There is still much work to be done. As models grow more complex and agents more autonomous, our debugging tools must evolve correspondingly. The open research directions we outlined beckon a future where AI debugging is a standard part of the development cycle, not an ad-hoc afterthought. In that future, developers might have **IDE-like environments for AI** where they can set watchpoints on a model’s knowledge, replay an agent’s conversation with different assumptions, or receive automatic suggestions for fixing a prompt that led to an error. Achieving this will require interdisciplinary collaboration – combining insights from software engineering, ML, HCI, and even fields like cognitive psychology (to better understand how to represent reasoning).

Ultimately, improving observability is about **building trust**. Trust that an LLM-based system will do the right thing, and if it doesn’t, we can understand why and fix it. In safety-critical domains (medicine, law, transportation), such trust is non-negotiable. Even in everyday applications, transparency can greatly enhance user experience and confidence.

By surveying the state-of-the-art and charting a path forward, we hope this paper inspires further research and development in making LLMs and multi-agent AI systems **transparent, predictable, and controllable**. As one speaker aptly put it, the goal is to move from *“flights blindfolded”* to *“flights with instruments”* when it comes to navigating the behavior of AI[medium.com](https://medium.com/@kpetropavlov/observability-in-multi-agent-llm-systems-telemetry-strategies-for-clarity-and-reliability-fafe9ca3780c#:~:text=Introduction). The instruments are being built – and with continued effort, the once-enigmatic processes of large language models will become as observable and steerable as any modern software component.

## References

1. **Rombaut, B.** *et al.* (2024). *Watson: A Cognitive Observability Framework for the Reasoning of LLM-Powered Agents*. ArXiv preprint arXiv:2411.03455[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=As%20foundation%20models%20,reasoning%20processes%20of%20agents%20driven)[arxiv.org](https://arxiv.org/html/2411.03455v2#:~:text=scenarios%20fast,agents%20in%20two%20scenarios%3A%20Massive).
2. **Zhou, Y.** *et al.* (2024). *LDB: A Large Language Model Debugger via Verifying Runtime Execution Step by Step*. ArXiv preprint arXiv:2402.16906[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=program%2C%20they%20do%20more%20than,2001)[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=visible%20test%20case%2C%20LDB%20segments,the%20quality%20of%20code%20generation).
3. **Chang, T. A.** *et al.* (2024). *Scalable Influence and Fact Tracing for Large Language Model Pretraining*. ArXiv preprint arXiv:2410.17413[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=promising%20results%20identifying%20examples%20that,2022%3B%20Chang%20%26%20Bergen%2C%202024)[arxiv.org](https://arxiv.org/html/2410.17413v1#:~:text=Researchers%20have%20proposed%20a%20variety,2023).
4. **Lee, A. W.** *et al.* (2025). *Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems*. Proceedings of VLDB 2025 (to appear)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=LLM%20constraints%E2%80%94also%20known%20as%20assertions,A%20related%20approach%20is%20constrained)[arxiv.org](https://arxiv.org/html/2503.00600v2#:~:text=2024a%20%2C%20%2026%3B%20Dong,2024).
5. **Hassan, A. E.** *et al.* (2024). *Rethinking Software Engineering in the Foundation Model Era: Challenges in Developing Trustworthy FMware*. ArXiv preprint arXiv:2402.15943[aiwarebootcamp.ioaiwarebootcamp.io](https://www.aiwarebootcamp.io/slides/2024_aiwarebootcamp_hassan_rethinking_software_and_se_in_the_fm_era.pdf#:~:text=C7%3A%20Operation%20%26%20semantic%20observability,C5%3A%20Compliance%20%26%20regulations%20Conclusion).
6. **Soatto, S.** *et al.* (2024). *Observability of Latent States in Generative AI Models*. Under review, ICLR 2025[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=the%20analysis%20must%20focus%20on,Brockett%2C%202015)[openreview.net](https://openreview.net/pdf?id=RaroYIrnbR#:~:text=match%20at%20L566%20observability%20for,tokens%20are%20observable%2C%20but%20simple).
7. **Anthropic (Claude team)** (2023). *Tracing the Thoughts of a Large Language Model*. *Anthropic Research Blog*, Oct 2023[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=,its%20fake%20reasoning%2C%20providing%20a)[anthropic.com](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=it%20only%20answers%20questions%20when,as%20models%20grow%20more%20sophisticated).
8. **Last9 (Sharma, P.)** (2023). *LangChain Observability: From Zero to Production in 10 Minutes*. *Last9 Engineering Blog*, June 2023[last9.io](https://last9.io/blog/langchain-observability/#:~:text=Add%20Tracing%20to%20FastAPI%20Endpoints)[last9.io](https://last9.io/blog/langchain-observability/#:~:text=def%20on_chain_start%28self%2C%20serialized%2C%20inputs%2C%20,chain_id%29%20except%20Exception%20as%20e).
9. **WitnessAI** (2023). *LLM Observability: How to Monitor and Optimize LLMs*. Witness AI Blog, Aug 2023[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Aspect%20LLM%20Monitoring%20LLM%20Observability,analysis%2C%20optimization%2C%20user%20behavior%20insights)[witness.ai](https://witness.ai/blog/llm-observability/#:~:text=Monitor%20Drift%2C%20Bias%2C%20Performance%2C%20%26,Data%20Integrity%20Issues).
10. **Owotogbe, J.** (2025). *Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering*. ArXiv preprint arXiv:2505.03096[arxiv.org](https://arxiv.org/html/2505.03096#:~:text=conditions,Although%20widely%20used%20in%20distributed)[arxiv.org](https://arxiv.org/html/2505.03096#:~:text=,distributed%20systems%20and%20alignment%20with).
11. **Liu, X.** *et al.* (2024). *REFLEXION: An Automated Debugging Agent for LLM-Generated Code*, in *NeurIPS 2024 Workshop on AI for Code*[arxiv.org](https://arxiv.org/html/2402.16906v1#:~:text=tests%20and%20program%20verifiers%20into,a%20novel%20debugging%20framework).
12. **Petropavlov, K.** (2025). *Observability in Multi-Agent LLM Systems: Telemetry Strategies for Clarity and Reliability*. Medium article, May 2025[medium.com](https://medium.com/@kpetropavlov/observability-in-multi-agent-llm-systems-telemetry-strategies-for-clarity-and-reliability-fafe9ca3780c#:~:text=Introduction).
